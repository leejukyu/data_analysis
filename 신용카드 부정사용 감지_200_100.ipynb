{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bebfd0",
   "metadata": {
    "id": "97bebfd0"
   },
   "source": [
    "## **1. Dataset 구성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52703e1b",
   "metadata": {
    "executionInfo": {
     "elapsed": 2047,
     "status": "ok",
     "timestamp": 1620908248591,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "52703e1b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from pandas import DataFrame\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = DataFrame(pd.read_csv('creditcard.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d549f39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1670,
     "status": "ok",
     "timestamp": 1620908248592,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "5d549f39",
    "outputId": "5b7ac8df-de9e-4b5f-d0c4-84a8b21143f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "# df.describe()\n",
    "# df.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fed5c7",
   "metadata": {},
   "source": [
    "### * **class 값 분석**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb10e7fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1126,
     "status": "ok",
     "timestamp": 1620908248592,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "cb10e7fa",
    "outputId": "da4d39cd-e61a-4552-9894-a4eab3f9474e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 %of the dateset\n",
      "Frauds 0.17 %of the dateset\n"
     ]
    }
   ],
   "source": [
    "print(\"No Frauds\", round(df[\"Class\"].value_counts()[0]/len(df) * 100,2),\n",
    "     \"%of the dateset\")\n",
    "print(\"Frauds\", round(df[\"Class\"].value_counts()[1]/len(df) * 100,2),\n",
    "     \"%of the dateset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81adf61f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1620908249134,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "81adf61f",
    "outputId": "e396d059-7ba0-4373-ea0d-ef8d3032e50a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = [\"#4374D9\", \"#F361A6\"]\n",
    "\n",
    "sns.countplot(\"Class\", data=df, palette=colors)\n",
    "plt.title(\"0: No Fraud || 1:Fraud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ded9ea8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df[df.columns[:-2]]\n",
    "Y=df['Class']\n",
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65676a4a",
   "metadata": {},
   "source": [
    "### * **StandardScaler Amount 값 정규화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59056a37",
   "metadata": {
    "executionInfo": {
     "elapsed": 697,
     "status": "ok",
     "timestamp": 1620908249476,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "59056a37"
   },
   "outputs": [],
   "source": [
    "dataset = df.values\n",
    "amount = dataset[:,29]\n",
    "amount = amount.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eadfbf2",
   "metadata": {
    "executionInfo": {
     "elapsed": 532,
     "status": "ok",
     "timestamp": 1620908249854,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "4eadfbf2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24496426],\n",
       "       [-0.34247454],\n",
       "       [ 1.16068593],\n",
       "       ...,\n",
       "       [-0.0818393 ],\n",
       "       [-0.31324853],\n",
       "       [ 0.51435531]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardScaler = StandardScaler()\n",
    "rob_scaler = RobustScaler()\n",
    "amount_data_standard = standardScaler.fit(amount).transform(amount)\n",
    "amount_data_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb26684",
   "metadata": {},
   "source": [
    "### * **Time 데이터 정규화 방법 분석**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46507aba",
   "metadata": {
    "executionInfo": {
     "elapsed": 680,
     "status": "ok",
     "timestamp": 1620908250459,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "46507aba"
   },
   "outputs": [],
   "source": [
    "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "df['loged_time'] = np.log1p(df['Time'].values.reshape(-1,1))\n",
    "df['std_time'] = standardScaler.fit_transform(df['Time'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9de12ec5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "executionInfo": {
     "elapsed": 5625,
     "status": "ok",
     "timestamp": 1620908255876,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "9de12ec5",
    "outputId": "fd5b6ca2-0218-4271-e5e6-db60f6f789d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.996583023457193, 1.6420577336572635)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(18,4))\n",
    "scaled_time = df['scaled_time'].values\n",
    "loged_time = df['loged_time'].values\n",
    "std_time = df['std_time'].values\n",
    "\n",
    "sns.distplot(scaled_time, ax=ax[0], color='r')\n",
    "ax[0].set_xlim(min(scaled_time), max(scaled_time))\n",
    "\n",
    "sns.distplot(loged_time, ax=ax[1], color='b')\n",
    "ax[1].set_xlim(min(loged_time), max(loged_time))\n",
    "\n",
    "sns.distplot(std_time, ax=ax[2], color='b')\n",
    "ax[2].set_xlim(min(std_time), max(std_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bea276",
   "metadata": {},
   "source": [
    "### * **RobustScaler를 활용한 Time 데이터 정규화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7eaba19",
   "metadata": {
    "executionInfo": {
     "elapsed": 5178,
     "status": "ok",
     "timestamp": 1620908255876,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "f7eaba19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.99498349],\n",
       "       [-0.99498349],\n",
       "       [-0.99497175],\n",
       "       ...,\n",
       "       [ 1.03497457],\n",
       "       [ 1.03497457],\n",
       "       [ 1.03502156]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df.values\n",
    "time_1 = dataset[:,0]\n",
    "time_1 = time_1.reshape(-1, 1)\n",
    "\n",
    "rob_scaler = RobustScaler()\n",
    "time_data_robust = rob_scaler.fit(time_1).transform(time_1)\n",
    "time_data_robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b64863f",
   "metadata": {
    "executionInfo": {
     "elapsed": 4763,
     "status": "ok",
     "timestamp": 1620908255877,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "0b64863f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.994983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.994983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.994972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.994972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.994960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>1.034951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>1.034963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>1.034975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>1.034975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>1.035022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        normalTime\n",
       "0        -0.994983\n",
       "1        -0.994983\n",
       "2        -0.994972\n",
       "3        -0.994972\n",
       "4        -0.994960\n",
       "...            ...\n",
       "284802    1.034951\n",
       "284803    1.034963\n",
       "284804    1.034975\n",
       "284805    1.034975\n",
       "284806    1.035022\n",
       "\n",
       "[284807 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amount_data = pd.DataFrame(amount_data_standard)\n",
    "time_data = pd.DataFrame(time_data_robust)\n",
    "time_data.columns = ['normalTime']\n",
    "time_data\n",
    "# print(type(time_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28060763",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 4393,
     "status": "ok",
     "timestamp": 1620908255877,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "28060763",
    "outputId": "b6dff9b9-03ba-41a5-a5ea-c6f850d6e266"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>-0.350151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>-0.254117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>-0.081839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>-0.313249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>0.514355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        normalAmount\n",
       "0           0.244964\n",
       "1          -0.342475\n",
       "2           1.160686\n",
       "3           0.140534\n",
       "4          -0.073403\n",
       "...              ...\n",
       "284802     -0.350151\n",
       "284803     -0.254117\n",
       "284804     -0.081839\n",
       "284805     -0.313249\n",
       "284806      0.514355\n",
       "\n",
       "[284807 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규화 된 Amount data 컬럼명 normalAmount로 지정\n",
    "amount_data.columns = ['normalAmount']\n",
    "amount_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51f03ef5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "executionInfo": {
     "elapsed": 3740,
     "status": "ok",
     "timestamp": 1620908255877,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "51f03ef5",
    "outputId": "169441e6-289d-4356-ea2d-92dbf11f81fd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>loged_time</th>\n",
       "      <th>std_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.996583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.996583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>-1.996562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>-1.996562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>-1.996541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>-0.371407</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>-1.996541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>-0.099254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994937</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>-1.996499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>1.249376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994901</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>-1.996436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>-0.410430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994901</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>-1.996436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>-0.366846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994878</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>-1.996394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "5 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "6  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
       "7 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
       "8 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
       "9 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
       "\n",
       "         V8        V9       V10  ...       V23       V24       V25       V26  \\\n",
       "0  0.098698  0.363787  0.090794  ... -0.110474  0.066928  0.128539 -0.189115   \n",
       "1  0.085102 -0.255425 -0.166974  ...  0.101288 -0.339846  0.167170  0.125895   \n",
       "2  0.247676 -1.514654  0.207643  ...  0.909412 -0.689281 -0.327642 -0.139097   \n",
       "3  0.377436 -1.387024 -0.054952  ... -0.190321 -1.175575  0.647376 -0.221929   \n",
       "4 -0.270533  0.817739  0.753074  ... -0.137458  0.141267 -0.206010  0.502292   \n",
       "5  0.260314 -0.568671 -0.371407  ... -0.026398 -0.371427 -0.232794  0.105915   \n",
       "6  0.081213  0.464960 -0.099254  ... -0.154104 -0.780055  0.750137 -0.257237   \n",
       "7 -3.807864  0.615375  1.249376  ...  0.057504 -0.649709 -0.415267 -0.051634   \n",
       "8  0.851084 -0.392048 -0.410430  ... -0.204233  1.011592  0.373205 -0.384157   \n",
       "9  0.069539 -0.736727 -0.366846  ... -0.120794 -0.385050 -0.069733  0.094199   \n",
       "\n",
       "        V27       V28  Class  scaled_time  loged_time  std_time  \n",
       "0  0.133558 -0.021053      0    -0.994983    0.000000 -1.996583  \n",
       "1 -0.008983  0.014724      0    -0.994983    0.000000 -1.996583  \n",
       "2 -0.055353 -0.059752      0    -0.994972    0.693147 -1.996562  \n",
       "3  0.062723  0.061458      0    -0.994972    0.693147 -1.996562  \n",
       "4  0.219422  0.215153      0    -0.994960    1.098612 -1.996541  \n",
       "5  0.253844  0.081080      0    -0.994960    1.098612 -1.996541  \n",
       "6  0.034507  0.005168      0    -0.994937    1.609438 -1.996499  \n",
       "7 -1.206921 -1.085339      0    -0.994901    2.079442 -1.996436  \n",
       "8  0.011747  0.142404      0    -0.994901    2.079442 -1.996436  \n",
       "9  0.246219  0.083076      0    -0.994878    2.302585 -1.996394  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존에 있던 Time, Amount 데이터 drop\n",
    "df.drop('Time', axis=1, inplace = True)\n",
    "df.drop('Amount', axis=1, inplace = True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6c9fad0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "executionInfo": {
     "elapsed": 1686,
     "status": "ok",
     "timestamp": 1620908256261,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "e6c9fad0",
    "outputId": "48ac4679-711f-4a82-d262-b49fa265d228",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalTime</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normalAmount</th>\n",
       "      <th>Class</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>loged_time</th>\n",
       "      <th>std_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.244964</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.996583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.342475</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.996583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.160686</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>-1.996562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.140534</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>-1.996562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.073403</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>-1.996541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.338556</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>-1.996541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.333279</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>-0.994937</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994937</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>-1.996499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.190107</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>-0.994901</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994901</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>-1.996436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.019392</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>-0.994901</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994901</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>-1.996436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.338516</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>-0.994878</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994878</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>-1.996394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   normalTime        V1        V2        V3        V4        V5        V6  \\\n",
       "0    0.244964 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1   -0.342475  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2    1.160686 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3    0.140534 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4   -0.073403 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "5   -0.338556 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728   \n",
       "6   -0.333279  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708   \n",
       "7   -0.190107 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118   \n",
       "8    0.019392 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818   \n",
       "9   -0.338516 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761   \n",
       "\n",
       "         V7        V8        V9  ...       V24       V25       V26       V27  \\\n",
       "0  0.239599  0.098698  0.363787  ...  0.066928  0.128539 -0.189115  0.133558   \n",
       "1 -0.078803  0.085102 -0.255425  ... -0.339846  0.167170  0.125895 -0.008983   \n",
       "2  0.791461  0.247676 -1.514654  ... -0.689281 -0.327642 -0.139097 -0.055353   \n",
       "3  0.237609  0.377436 -1.387024  ... -1.175575  0.647376 -0.221929  0.062723   \n",
       "4  0.592941 -0.270533  0.817739  ...  0.141267 -0.206010  0.502292  0.219422   \n",
       "5  0.476201  0.260314 -0.568671  ... -0.371427 -0.232794  0.105915  0.253844   \n",
       "6 -0.005159  0.081213  0.464960  ... -0.780055  0.750137 -0.257237  0.034507   \n",
       "7  1.120631 -3.807864  0.615375  ... -0.649709 -0.415267 -0.051634 -1.206921   \n",
       "8  0.370145  0.851084 -0.392048  ...  1.011592  0.373205 -0.384157  0.011747   \n",
       "9  0.651583  0.069539 -0.736727  ... -0.385050 -0.069733  0.094199  0.246219   \n",
       "\n",
       "        V28  normalAmount  Class  scaled_time  loged_time  std_time  \n",
       "0 -0.021053     -0.994983      0    -0.994983    0.000000 -1.996583  \n",
       "1  0.014724     -0.994983      0    -0.994983    0.000000 -1.996583  \n",
       "2 -0.059752     -0.994972      0    -0.994972    0.693147 -1.996562  \n",
       "3  0.061458     -0.994972      0    -0.994972    0.693147 -1.996562  \n",
       "4  0.215153     -0.994960      0    -0.994960    1.098612 -1.996541  \n",
       "5  0.081080     -0.994960      0    -0.994960    1.098612 -1.996541  \n",
       "6  0.005168     -0.994937      0    -0.994937    1.609438 -1.996499  \n",
       "7 -1.085339     -0.994901      0    -0.994901    2.079442 -1.996436  \n",
       "8  0.142404     -0.994901      0    -0.994901    2.079442 -1.996436  \n",
       "9  0.083076     -0.994878      0    -0.994878    2.302585 -1.996394  \n",
       "\n",
       "[10 rows x 34 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규화된 Time,Amount data 삽입\n",
    "df.insert(0,'normalTime', amount_data )\n",
    "df.insert(29,'normalAmount',time_data)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c53a8202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalTime</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normalAmount</th>\n",
       "      <th>Class</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>loged_time</th>\n",
       "      <th>std_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123141</th>\n",
       "      <td>0.665323</td>\n",
       "      <td>-6.616293</td>\n",
       "      <td>3.563428</td>\n",
       "      <td>-7.058901</td>\n",
       "      <td>4.284346</td>\n",
       "      <td>-5.096299</td>\n",
       "      <td>-1.768618</td>\n",
       "      <td>-4.937554</td>\n",
       "      <td>2.748460</td>\n",
       "      <td>-3.796760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275662</td>\n",
       "      <td>0.057425</td>\n",
       "      <td>-0.265838</td>\n",
       "      <td>-0.514637</td>\n",
       "      <td>0.388590</td>\n",
       "      <td>-0.092412</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.092412</td>\n",
       "      <td>11.249311</td>\n",
       "      <td>-0.378787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42887</th>\n",
       "      <td>-0.204021</td>\n",
       "      <td>-12.835760</td>\n",
       "      <td>6.574615</td>\n",
       "      <td>-12.788462</td>\n",
       "      <td>8.786257</td>\n",
       "      <td>-10.723121</td>\n",
       "      <td>-2.813536</td>\n",
       "      <td>-14.248847</td>\n",
       "      <td>7.960521</td>\n",
       "      <td>-7.718751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625349</td>\n",
       "      <td>0.125865</td>\n",
       "      <td>0.177624</td>\n",
       "      <td>-0.817680</td>\n",
       "      <td>-0.521030</td>\n",
       "      <td>-0.509957</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.509957</td>\n",
       "      <td>10.628279</td>\n",
       "      <td>-1.127207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151730</th>\n",
       "      <td>-0.277426</td>\n",
       "      <td>-1.952933</td>\n",
       "      <td>3.541385</td>\n",
       "      <td>-1.310561</td>\n",
       "      <td>5.955664</td>\n",
       "      <td>-1.003993</td>\n",
       "      <td>0.983049</td>\n",
       "      <td>-4.587235</td>\n",
       "      <td>-4.892184</td>\n",
       "      <td>-2.516752</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215379</td>\n",
       "      <td>-0.865599</td>\n",
       "      <td>0.212545</td>\n",
       "      <td>0.532897</td>\n",
       "      <td>0.357892</td>\n",
       "      <td>0.134435</td>\n",
       "      <td>1</td>\n",
       "      <td>0.134435</td>\n",
       "      <td>11.473519</td>\n",
       "      <td>0.027820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30496</th>\n",
       "      <td>0.093357</td>\n",
       "      <td>-4.844372</td>\n",
       "      <td>5.649439</td>\n",
       "      <td>-6.730396</td>\n",
       "      <td>5.252842</td>\n",
       "      <td>-4.409566</td>\n",
       "      <td>-1.740767</td>\n",
       "      <td>-6.311699</td>\n",
       "      <td>3.449167</td>\n",
       "      <td>-5.416284</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216443</td>\n",
       "      <td>-0.325033</td>\n",
       "      <td>-0.270328</td>\n",
       "      <td>0.210214</td>\n",
       "      <td>0.391855</td>\n",
       "      <td>-0.572598</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.572598</td>\n",
       "      <td>10.489996</td>\n",
       "      <td>-1.239487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149869</th>\n",
       "      <td>-0.281264</td>\n",
       "      <td>-1.108478</td>\n",
       "      <td>3.448953</td>\n",
       "      <td>-6.216972</td>\n",
       "      <td>3.021052</td>\n",
       "      <td>-0.529901</td>\n",
       "      <td>-2.551375</td>\n",
       "      <td>-2.001743</td>\n",
       "      <td>1.092432</td>\n",
       "      <td>-0.836098</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.295497</td>\n",
       "      <td>-0.690232</td>\n",
       "      <td>-0.364749</td>\n",
       "      <td>0.229327</td>\n",
       "      <td>0.208830</td>\n",
       "      <td>0.086937</td>\n",
       "      <td>1</td>\n",
       "      <td>0.086937</td>\n",
       "      <td>11.430554</td>\n",
       "      <td>-0.057317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30384</th>\n",
       "      <td>-0.349231</td>\n",
       "      <td>-2.857170</td>\n",
       "      <td>4.045601</td>\n",
       "      <td>-4.197299</td>\n",
       "      <td>5.487199</td>\n",
       "      <td>-3.070776</td>\n",
       "      <td>-1.422686</td>\n",
       "      <td>-5.651314</td>\n",
       "      <td>2.019657</td>\n",
       "      <td>-5.015491</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067195</td>\n",
       "      <td>-0.476931</td>\n",
       "      <td>-0.103716</td>\n",
       "      <td>1.166961</td>\n",
       "      <td>0.663632</td>\n",
       "      <td>-0.573233</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.573233</td>\n",
       "      <td>10.488493</td>\n",
       "      <td>-1.240625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220725</th>\n",
       "      <td>0.944509</td>\n",
       "      <td>-1.169203</td>\n",
       "      <td>1.863414</td>\n",
       "      <td>-2.515135</td>\n",
       "      <td>5.463681</td>\n",
       "      <td>-0.297971</td>\n",
       "      <td>1.364918</td>\n",
       "      <td>0.759219</td>\n",
       "      <td>-0.118861</td>\n",
       "      <td>-2.293921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078616</td>\n",
       "      <td>-0.544655</td>\n",
       "      <td>0.014777</td>\n",
       "      <td>-0.240930</td>\n",
       "      <td>-0.781055</td>\n",
       "      <td>0.676559</td>\n",
       "      <td>1</td>\n",
       "      <td>0.676559</td>\n",
       "      <td>11.865559</td>\n",
       "      <td>0.999538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119781</th>\n",
       "      <td>0.144652</td>\n",
       "      <td>-2.866364</td>\n",
       "      <td>2.346949</td>\n",
       "      <td>-4.053307</td>\n",
       "      <td>3.983359</td>\n",
       "      <td>-3.463186</td>\n",
       "      <td>-1.280953</td>\n",
       "      <td>-4.474764</td>\n",
       "      <td>1.216655</td>\n",
       "      <td>-2.309829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282030</td>\n",
       "      <td>-0.506901</td>\n",
       "      <td>-0.371741</td>\n",
       "      <td>0.615257</td>\n",
       "      <td>0.803163</td>\n",
       "      <td>-0.107038</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.107038</td>\n",
       "      <td>11.232973</td>\n",
       "      <td>-0.405004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42009</th>\n",
       "      <td>0.095876</td>\n",
       "      <td>-2.740483</td>\n",
       "      <td>3.658095</td>\n",
       "      <td>-4.110636</td>\n",
       "      <td>5.340242</td>\n",
       "      <td>-2.666775</td>\n",
       "      <td>-0.092782</td>\n",
       "      <td>-4.388699</td>\n",
       "      <td>-0.280133</td>\n",
       "      <td>-2.821895</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154757</td>\n",
       "      <td>-0.403956</td>\n",
       "      <td>0.277895</td>\n",
       "      <td>0.830062</td>\n",
       "      <td>0.218690</td>\n",
       "      <td>-0.514257</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.514257</td>\n",
       "      <td>10.619374</td>\n",
       "      <td>-1.134914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43428</th>\n",
       "      <td>1.102834</td>\n",
       "      <td>-16.526507</td>\n",
       "      <td>8.584972</td>\n",
       "      <td>-18.649853</td>\n",
       "      <td>9.505594</td>\n",
       "      <td>-13.793819</td>\n",
       "      <td>-2.832404</td>\n",
       "      <td>-16.701694</td>\n",
       "      <td>7.517344</td>\n",
       "      <td>-8.507059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673461</td>\n",
       "      <td>-1.413700</td>\n",
       "      <td>-0.462762</td>\n",
       "      <td>-2.018575</td>\n",
       "      <td>-1.042804</td>\n",
       "      <td>-0.507372</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.507372</td>\n",
       "      <td>10.633593</td>\n",
       "      <td>-1.122574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124087</th>\n",
       "      <td>-0.349231</td>\n",
       "      <td>1.118560</td>\n",
       "      <td>1.291858</td>\n",
       "      <td>-1.298805</td>\n",
       "      <td>2.135772</td>\n",
       "      <td>0.772204</td>\n",
       "      <td>-1.147291</td>\n",
       "      <td>0.390578</td>\n",
       "      <td>-0.107072</td>\n",
       "      <td>-0.038339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017911</td>\n",
       "      <td>0.650302</td>\n",
       "      <td>-0.332366</td>\n",
       "      <td>0.105949</td>\n",
       "      <td>0.128124</td>\n",
       "      <td>-0.088359</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.088359</td>\n",
       "      <td>11.253792</td>\n",
       "      <td>-0.371522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149874</th>\n",
       "      <td>2.048541</td>\n",
       "      <td>-1.662937</td>\n",
       "      <td>3.253892</td>\n",
       "      <td>-7.040485</td>\n",
       "      <td>2.266456</td>\n",
       "      <td>-4.177649</td>\n",
       "      <td>-0.746925</td>\n",
       "      <td>-0.248337</td>\n",
       "      <td>1.091157</td>\n",
       "      <td>-0.307137</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.318012</td>\n",
       "      <td>-1.255362</td>\n",
       "      <td>-0.691963</td>\n",
       "      <td>0.264878</td>\n",
       "      <td>-0.130445</td>\n",
       "      <td>0.087055</td>\n",
       "      <td>1</td>\n",
       "      <td>0.087055</td>\n",
       "      <td>11.430663</td>\n",
       "      <td>-0.057106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238222</th>\n",
       "      <td>-0.348831</td>\n",
       "      <td>-4.280584</td>\n",
       "      <td>1.421100</td>\n",
       "      <td>-3.908229</td>\n",
       "      <td>2.942946</td>\n",
       "      <td>-0.076205</td>\n",
       "      <td>-2.002526</td>\n",
       "      <td>-2.874155</td>\n",
       "      <td>-0.856005</td>\n",
       "      <td>0.963674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349170</td>\n",
       "      <td>0.056276</td>\n",
       "      <td>-1.149923</td>\n",
       "      <td>-1.809886</td>\n",
       "      <td>0.723051</td>\n",
       "      <td>0.762344</td>\n",
       "      <td>1</td>\n",
       "      <td>0.762344</td>\n",
       "      <td>11.915607</td>\n",
       "      <td>1.153303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204079</th>\n",
       "      <td>0.080603</td>\n",
       "      <td>1.862102</td>\n",
       "      <td>-0.124052</td>\n",
       "      <td>-1.989752</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.473032</td>\n",
       "      <td>-0.674517</td>\n",
       "      <td>0.298621</td>\n",
       "      <td>-0.282416</td>\n",
       "      <td>0.802053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388335</td>\n",
       "      <td>0.007896</td>\n",
       "      <td>-0.120980</td>\n",
       "      <td>-0.019579</td>\n",
       "      <td>0.006155</td>\n",
       "      <td>0.592230</td>\n",
       "      <td>1</td>\n",
       "      <td>0.592230</td>\n",
       "      <td>11.813793</td>\n",
       "      <td>0.848385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275992</th>\n",
       "      <td>2.182757</td>\n",
       "      <td>-2.027135</td>\n",
       "      <td>-1.131890</td>\n",
       "      <td>-1.135194</td>\n",
       "      <td>1.086963</td>\n",
       "      <td>-0.010547</td>\n",
       "      <td>0.423797</td>\n",
       "      <td>3.790880</td>\n",
       "      <td>-1.155595</td>\n",
       "      <td>-0.063434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756502</td>\n",
       "      <td>-0.142685</td>\n",
       "      <td>-0.602777</td>\n",
       "      <td>0.508712</td>\n",
       "      <td>-0.091646</td>\n",
       "      <td>0.964990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.964990</td>\n",
       "      <td>12.024743</td>\n",
       "      <td>1.516531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150654</th>\n",
       "      <td>-0.353189</td>\n",
       "      <td>-3.765680</td>\n",
       "      <td>5.890735</td>\n",
       "      <td>-10.202268</td>\n",
       "      <td>10.259036</td>\n",
       "      <td>-5.611448</td>\n",
       "      <td>-3.235376</td>\n",
       "      <td>-10.632683</td>\n",
       "      <td>3.272716</td>\n",
       "      <td>-5.268905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382025</td>\n",
       "      <td>-0.821036</td>\n",
       "      <td>0.394355</td>\n",
       "      <td>1.412961</td>\n",
       "      <td>0.782407</td>\n",
       "      <td>0.107403</td>\n",
       "      <td>1</td>\n",
       "      <td>0.107403</td>\n",
       "      <td>11.449293</td>\n",
       "      <td>-0.020634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42958</th>\n",
       "      <td>-0.313289</td>\n",
       "      <td>-13.897206</td>\n",
       "      <td>6.344280</td>\n",
       "      <td>-14.281666</td>\n",
       "      <td>5.581009</td>\n",
       "      <td>-12.887133</td>\n",
       "      <td>-3.146176</td>\n",
       "      <td>-15.450467</td>\n",
       "      <td>9.060281</td>\n",
       "      <td>-5.486121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.763508</td>\n",
       "      <td>0.075456</td>\n",
       "      <td>-0.453840</td>\n",
       "      <td>-1.508968</td>\n",
       "      <td>-0.686836</td>\n",
       "      <td>-0.509628</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.509628</td>\n",
       "      <td>10.628957</td>\n",
       "      <td>-1.126617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249607</th>\n",
       "      <td>-0.213296</td>\n",
       "      <td>-7.381547</td>\n",
       "      <td>-7.449015</td>\n",
       "      <td>-4.696287</td>\n",
       "      <td>3.728439</td>\n",
       "      <td>6.198304</td>\n",
       "      <td>-6.406267</td>\n",
       "      <td>-5.831452</td>\n",
       "      <td>1.457175</td>\n",
       "      <td>-0.646203</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.635874</td>\n",
       "      <td>0.123539</td>\n",
       "      <td>0.404729</td>\n",
       "      <td>0.704915</td>\n",
       "      <td>-1.229992</td>\n",
       "      <td>0.820040</td>\n",
       "      <td>1</td>\n",
       "      <td>0.820040</td>\n",
       "      <td>11.947911</td>\n",
       "      <td>1.256719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149357</th>\n",
       "      <td>-0.198264</td>\n",
       "      <td>-1.855061</td>\n",
       "      <td>1.554964</td>\n",
       "      <td>-1.405809</td>\n",
       "      <td>0.669327</td>\n",
       "      <td>-0.280230</td>\n",
       "      <td>1.178652</td>\n",
       "      <td>-3.459979</td>\n",
       "      <td>-2.815155</td>\n",
       "      <td>1.242229</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.368344</td>\n",
       "      <td>0.198731</td>\n",
       "      <td>-0.078591</td>\n",
       "      <td>0.366709</td>\n",
       "      <td>0.073767</td>\n",
       "      <td>0.074989</td>\n",
       "      <td>1</td>\n",
       "      <td>0.074989</td>\n",
       "      <td>11.419450</td>\n",
       "      <td>-0.078733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93486</th>\n",
       "      <td>-0.353229</td>\n",
       "      <td>1.079524</td>\n",
       "      <td>0.872988</td>\n",
       "      <td>-0.303850</td>\n",
       "      <td>2.755369</td>\n",
       "      <td>0.301688</td>\n",
       "      <td>-0.350284</td>\n",
       "      <td>-0.042848</td>\n",
       "      <td>0.246625</td>\n",
       "      <td>-0.779176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060327</td>\n",
       "      <td>0.358339</td>\n",
       "      <td>0.076984</td>\n",
       "      <td>0.018936</td>\n",
       "      <td>0.060574</td>\n",
       "      <td>-0.237890</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.237890</td>\n",
       "      <td>11.073552</td>\n",
       "      <td>-0.639547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        normalTime         V1        V2         V3         V4         V5  \\\n",
       "123141    0.665323  -6.616293  3.563428  -7.058901   4.284346  -5.096299   \n",
       "42887    -0.204021 -12.835760  6.574615 -12.788462   8.786257 -10.723121   \n",
       "151730   -0.277426  -1.952933  3.541385  -1.310561   5.955664  -1.003993   \n",
       "30496     0.093357  -4.844372  5.649439  -6.730396   5.252842  -4.409566   \n",
       "149869   -0.281264  -1.108478  3.448953  -6.216972   3.021052  -0.529901   \n",
       "30384    -0.349231  -2.857170  4.045601  -4.197299   5.487199  -3.070776   \n",
       "220725    0.944509  -1.169203  1.863414  -2.515135   5.463681  -0.297971   \n",
       "119781    0.144652  -2.866364  2.346949  -4.053307   3.983359  -3.463186   \n",
       "42009     0.095876  -2.740483  3.658095  -4.110636   5.340242  -2.666775   \n",
       "43428     1.102834 -16.526507  8.584972 -18.649853   9.505594 -13.793819   \n",
       "124087   -0.349231   1.118560  1.291858  -1.298805   2.135772   0.772204   \n",
       "149874    2.048541  -1.662937  3.253892  -7.040485   2.266456  -4.177649   \n",
       "238222   -0.348831  -4.280584  1.421100  -3.908229   2.942946  -0.076205   \n",
       "204079    0.080603   1.862102 -0.124052  -1.989752   0.382609   0.473032   \n",
       "275992    2.182757  -2.027135 -1.131890  -1.135194   1.086963  -0.010547   \n",
       "150654   -0.353189  -3.765680  5.890735 -10.202268  10.259036  -5.611448   \n",
       "42958    -0.313289 -13.897206  6.344280 -14.281666   5.581009 -12.887133   \n",
       "249607   -0.213296  -7.381547 -7.449015  -4.696287   3.728439   6.198304   \n",
       "149357   -0.198264  -1.855061  1.554964  -1.405809   0.669327  -0.280230   \n",
       "93486    -0.353229   1.079524  0.872988  -0.303850   2.755369   0.301688   \n",
       "\n",
       "              V6         V7        V8        V9  ...       V24       V25  \\\n",
       "123141 -1.768618  -4.937554  2.748460 -3.796760  ...  0.275662  0.057425   \n",
       "42887  -2.813536 -14.248847  7.960521 -7.718751  ...  0.625349  0.125865   \n",
       "151730  0.983049  -4.587235 -4.892184 -2.516752  ... -0.215379 -0.865599   \n",
       "30496  -1.740767  -6.311699  3.449167 -5.416284  ... -0.216443 -0.325033   \n",
       "149869 -2.551375  -2.001743  1.092432 -0.836098  ... -0.295497 -0.690232   \n",
       "30384  -1.422686  -5.651314  2.019657 -5.015491  ... -0.067195 -0.476931   \n",
       "220725  1.364918   0.759219 -0.118861 -2.293921  ... -0.078616 -0.544655   \n",
       "119781 -1.280953  -4.474764  1.216655 -2.309829  ...  0.282030 -0.506901   \n",
       "42009  -0.092782  -4.388699 -0.280133 -2.821895  ... -0.154757 -0.403956   \n",
       "43428  -2.832404 -16.701694  7.517344 -8.507059  ...  0.673461 -1.413700   \n",
       "124087 -1.147291   0.390578 -0.107072 -0.038339  ...  0.017911  0.650302   \n",
       "149874 -0.746925  -0.248337  1.091157 -0.307137  ... -0.318012 -1.255362   \n",
       "238222 -2.002526  -2.874155 -0.856005  0.963674  ... -0.349170  0.056276   \n",
       "204079 -0.674517   0.298621 -0.282416  0.802053  ...  0.388335  0.007896   \n",
       "275992  0.423797   3.790880 -1.155595 -0.063434  ...  0.756502 -0.142685   \n",
       "150654 -3.235376 -10.632683  3.272716 -5.268905  ...  0.382025 -0.821036   \n",
       "42958  -3.146176 -15.450467  9.060281 -5.486121  ...  0.763508  0.075456   \n",
       "249607 -6.406267  -5.831452  1.457175 -0.646203  ... -0.635874  0.123539   \n",
       "149357  1.178652  -3.459979 -2.815155  1.242229  ... -0.368344  0.198731   \n",
       "93486  -0.350284  -0.042848  0.246625 -0.779176  ... -0.060327  0.358339   \n",
       "\n",
       "             V26       V27       V28  normalAmount  Class  scaled_time  \\\n",
       "123141 -0.265838 -0.514637  0.388590     -0.092412      1    -0.092412   \n",
       "42887   0.177624 -0.817680 -0.521030     -0.509957      1    -0.509957   \n",
       "151730  0.212545  0.532897  0.357892      0.134435      1     0.134435   \n",
       "30496  -0.270328  0.210214  0.391855     -0.572598      1    -0.572598   \n",
       "149869 -0.364749  0.229327  0.208830      0.086937      1     0.086937   \n",
       "30384  -0.103716  1.166961  0.663632     -0.573233      1    -0.573233   \n",
       "220725  0.014777 -0.240930 -0.781055      0.676559      1     0.676559   \n",
       "119781 -0.371741  0.615257  0.803163     -0.107038      1    -0.107038   \n",
       "42009   0.277895  0.830062  0.218690     -0.514257      1    -0.514257   \n",
       "43428  -0.462762 -2.018575 -1.042804     -0.507372      1    -0.507372   \n",
       "124087 -0.332366  0.105949  0.128124     -0.088359      1    -0.088359   \n",
       "149874 -0.691963  0.264878 -0.130445      0.087055      1     0.087055   \n",
       "238222 -1.149923 -1.809886  0.723051      0.762344      1     0.762344   \n",
       "204079 -0.120980 -0.019579  0.006155      0.592230      1     0.592230   \n",
       "275992 -0.602777  0.508712 -0.091646      0.964990      1     0.964990   \n",
       "150654  0.394355  1.412961  0.782407      0.107403      1     0.107403   \n",
       "42958  -0.453840 -1.508968 -0.686836     -0.509628      1    -0.509628   \n",
       "249607  0.404729  0.704915 -1.229992      0.820040      1     0.820040   \n",
       "149357 -0.078591  0.366709  0.073767      0.074989      1     0.074989   \n",
       "93486   0.076984  0.018936  0.060574     -0.237890      1    -0.237890   \n",
       "\n",
       "        loged_time  std_time  \n",
       "123141   11.249311 -0.378787  \n",
       "42887    10.628279 -1.127207  \n",
       "151730   11.473519  0.027820  \n",
       "30496    10.489996 -1.239487  \n",
       "149869   11.430554 -0.057317  \n",
       "30384    10.488493 -1.240625  \n",
       "220725   11.865559  0.999538  \n",
       "119781   11.232973 -0.405004  \n",
       "42009    10.619374 -1.134914  \n",
       "43428    10.633593 -1.122574  \n",
       "124087   11.253792 -0.371522  \n",
       "149874   11.430663 -0.057106  \n",
       "238222   11.915607  1.153303  \n",
       "204079   11.813793  0.848385  \n",
       "275992   12.024743  1.516531  \n",
       "150654   11.449293 -0.020634  \n",
       "42958    10.628957 -1.126617  \n",
       "249607   11.947911  1.256719  \n",
       "149357   11.419450 -0.078733  \n",
       "93486    11.073552 -0.639547  \n",
       "\n",
       "[20 rows x 34 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1)\n",
    "\n",
    "fraud_df = df.loc[df['Class'] == 1]\n",
    "non_fraud_df  = df.loc[df['Class'] == 0][492:]\n",
    "\n",
    "normal_distributed_df= pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "df = normal_distributed_df.sample(frac=1, random_state=42)\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04aa371e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df[df.columns[:-2]]\n",
    "Y=df['Class']\n",
    "\n",
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44a77a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAASRElEQVR4nO3df7DddX3n8efLBERbIWCyFBM0to3dYTuVsiml2h8q/eGPtrCtsugqqaWTbes6te5ui9XWH1M6dabWqrV2UkGCuyrZUiTbMlYGZV07/goVkB9VUxYkWSRXfgm4bEXf/eN87qeHcJOchHzPucl9PmbOnO/3/f2c732fmTv3db+/U1VIkgTwuFk3IElaPAwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxSkgSR5TpIde1l+UZJf2tP8oWhf31mLn6GgQSQ5LsllSR5McluSl+3HZ9+UpJKcNVZb3mprD6CXNyX5ZpIHxl6/tb/rmbUkm5J8Mcm39xUeh8t31vQtn3UDOmy9G/gn4HjgZOBvklxXVTdO+Pm7gTcnubSqvnUQ+rmkql6+twFJlh2knzWU64BLgLdOOP5w+M6aMrcUdNAl+Q7gF4HfraoHquqTwFbgFfuxmo8wCpUF/6glOSbJxUnm2pbIG5Ls1+9z213zniRXJHkQeG6SFyX5fJKvJ7k9yZvGxj9q10iSW5P8ZJt+QlvnPUluAn5of/rZl6p6d1VdBTx0oOs41L6zps9Q0BCeATxcVV8aq10H/BuAJE9Ncm+Sp+5lHQX8LvDGJEcssPxdwDHAdwM/AZwDvPIAen0ZcD7wJOCTwINtXSuAFwG/luTMCdf1RuB72utngA0H0M8BSfKjSe6dcPhh8Z01DENBQ/hO4Ou71e5j9EeIqvpKVa2oqq/sbSVVtRWYA35lvJ5kGXA28Lqqur+qbgXext63RM5qQTT/ekqrX15Vf1dV366qh6rq6qr6Qpu/Hvggo9CZxFnA+VV1d1XdDrxzws89ZlX1yapasXs/h/N31jAMBQ3hAeDo3WpHA/cfwLreALweOGqsthI4ArhtrHYbsHov69nSgmj+9X9b/fbxQUl+OMnH226p+4BfbT9vEk/ZbX237WnglCzF76zHyFDQEL4ELE+ybqz2TGDSg8xdVV0JbAd+faz8NeCbwNPGak8Fdu5/q+x+7/gPMDr+cWJVHQP8OZC27EHgifMD2xbLqrHP3gGcuFtPi9FS/M6akKGgg66qHgT+CnhLku9I8mzgDOD9B7jK1wP9dMp2tswW4PwkT0ryNOC1wH97bJ0Do11cd1fVQ0lOZbT/fd6XgKPagdkjGG3FPH5s+RbgdUmOTbIGePVB6KdLcmSSoxj9wT4iyVH7e3B9Dxbtd9b0GQoayq8DTwB2MdpH/Wvzp6O2A80P7ONAc1dVfwd8drfyqxn9F3sLo4OlHwAuPEh9vyXJ/cDvMfqjN9/HfW35exltlTwIjJ+Z82ZGu0/+D/BRDjwE9+SjwP8DngVsatM/DpDkx5I8cIDrXczfWVMWn7wmzUaSi4Crq+qihealWXBLQZLUeUWzNDsfBm7dy7w0de4+kiR1h/SWwsqVK2vt2rWzbkOSDinXXHPN16pq1ULLDulQWLt2Ldu2bZt1G5J0SEmyx4sMPdAsSeoMBUlSN2gotFvsfiHJtUm2tdpxSa5M8uX2fmyrJ8k7k2xPcn2SU4bsTZL0aNPYUnhuVZ1cVevb/HnAVVW1DriqzQO8AFjXXhuB90yhN0nSmFnsPjoD2NymNwNnjtUvrpFPAyuSnDCD/iRpyRo6FAr4aJJrkmxsteOr6o42/VVGj2uE0W2Px2/Bu4MFboWcZGOSbUm2zc3NDdW3JC1JQ5+S+qNVtTPJvwKuTPIP4wurqpLs19VzVbWJ0c3AWL9+vVfeSdJBNOiWQlXtbO+7gMuAU4E753cLtfddbfhOHnlf9jUc2P3xJUkHaLBQaPfRf9L8NPDTwA2MHuYx/xzXDcDlbXorcE47C+k04L6x3UySpCkYcvfR8cBlSeZ/zgeq6iNJPgdsSXIuo/uwn9XGXwG8kNFTtr7BgT2Efb/9+9d/aho/RoeYS87/kVm3wFf+y4dm3YIWoaf+0dmDrn+wUKiqWxg9gnH3+l3A6QvUC3jVUP1IkvbNK5olSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJ3eChkGRZks8n+es2//Qkn0myPcklSY5s9ce3+e1t+dqhe5MkPdI0thR+A7h5bP6twNur6nuBe4BzW/1c4J5Wf3sbJ0maokFDIcka4EXAe9t8gOcBf9mGbAbObNNntHna8tPbeEnSlAy9pfAnwG8B327zTwburaqH2/wOYHWbXg3cDtCW39fGP0KSjUm2Jdk2Nzc3YOuStPQMFgpJfhbYVVXXHMz1VtWmqlpfVetXrVp1MFctSUve8gHX/Wzg55O8EDgKOBp4B7AiyfK2NbAG2NnG7wROBHYkWQ4cA9w1YH+SpN0MtqVQVa+rqjVVtRY4G/hYVf0H4OPAi9uwDcDlbXprm6ct/1hV1VD9SZIebRbXKfw28Nok2xkdM7ig1S8AntzqrwXOm0FvkrSkDbn7qKuqq4Gr2/QtwKkLjHkIeMk0+pEkLcwrmiVJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEndYKGQ5Kgkn01yXZIbk7y51Z+e5DNJtie5JMmRrf74Nr+9LV87VG+SpIUNuaXw/4HnVdUzgZOB5yc5DXgr8Paq+l7gHuDcNv5c4J5Wf3sbJ0maosFCoUYeaLNHtFcBzwP+stU3A2e26TPaPG356UkyVH+SpEcb9JhCkmVJrgV2AVcC/wjcW1UPtyE7gNVtejVwO0Bbfh/w5CH7kyQ90qChUFXfqqqTgTXAqcC/fqzrTLIxybYk2+bm5h7r6iRJY6Zy9lFV3Qt8HPgRYEWS5W3RGmBnm94JnAjQlh8D3LXAujZV1fqqWr9q1aqhW5ekJWXIs49WJVnRpp8A/BRwM6NweHEbtgG4vE1vbfO05R+rqhqqP0nSoy3f95ADdgKwOckyRuGzpar+OslNwIeS/D7weeCCNv4C4P1JtgN3A2cP2JskaQGDhUJVXQ/84AL1WxgdX9i9/hDwkqH6kSTtm1c0S5K6iUIhyVWT1CRJh7a97j5KchTwRGBlkmOB+YvJjuZfri+QJB0m9nVM4T8CrwGeAlzDv4TC14E/Ha4tSdIs7DUUquodwDuSvLqq3jWlniRJMzLR2UdV9a4kzwLWjn+mqi4eqC9J0gxMFApJ3g98D3At8K1WLsBQkKTDyKTXKawHTvIKY0k6vE16ncINwHcN2YgkafYm3VJYCdyU5LOMHp4DQFX9/CBdSZJmYtJQeNOQTUiSFodJzz76X0M3IkmavUnPPrqf0dlGAEcyerTmg1V19FCNSZKmb9IthSfNT7fnJp8BnDZUU5Kk2djvu6TWyIeBnzn47UiSZmnS3Ue/MDb7OEbXLTw0SEeSpJmZ9Oyjnxubfhi4ldEuJEnSYWTSYwqvHLoRSdLsTfqQnTVJLkuyq70uTbJm6OYkSdM16YHm9wFbGT1X4SnA/2w1SdJhZNJQWFVV76uqh9vrImDVgH1JkmZg0lC4K8nLkyxrr5cDdw3ZmCRp+iYNhV8GzgK+CtwBvBj4pYF6kiTNyKSnpL4F2FBV9wAkOQ74I0ZhIUk6TEy6pfAD84EAUFV3Az84TEuSpFmZNBQel+TY+Zm2pTDpVoYk6RAx6R/2twGfSvI/2vxLgPOHaUmSNCuTXtF8cZJtwPNa6Req6qbh2pIkzcLEu4BaCBgEknQY2+9bZ0uSDl+GgiSpMxQkSZ2hIEnqBguFJCcm+XiSm5LcmOQ3Wv24JFcm+XJ7P7bVk+SdSbYnuT7JKUP1Jkla2JBbCg8D/7mqTgJOA16V5CTgPOCqqloHXNXmAV4ArGuvjcB7BuxNkrSAwUKhqu6oqr9v0/cDNwOrGT3Gc3Mbthk4s02fAVxcI58GViQ5Yaj+JEmPNpVjCknWMrpX0meA46vqjrboq8DxbXo1cPvYx3a0miRpSgYPhSTfCVwKvKaqvj6+rKoKqP1c38Yk25Jsm5ubO4idSpIGDYUkRzAKhP9eVX/VynfO7xZq77tafSdw4tjH17TaI1TVpqpaX1XrV63y4W+SdDANefZRgAuAm6vqj8cWbQU2tOkNwOVj9XPaWUinAfeN7WaSJE3BkLe/fjbwCuALSa5ttd8B/hDYkuRc4DZGT3QDuAJ4IbAd+AbwygF7kyQtYLBQqKpPAtnD4tMXGF/Aq4bqR5K0b17RLEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSN1goJLkwya4kN4zVjktyZZIvt/djWz1J3plke5Lrk5wyVF+SpD0bckvhIuD5u9XOA66qqnXAVW0e4AXAuvbaCLxnwL4kSXswWChU1SeAu3crnwFsbtObgTPH6hfXyKeBFUlOGKo3SdLCpn1M4fiquqNNfxU4vk2vBm4fG7ej1R4lycYk25Jsm5ubG65TSVqCZnaguaoKqAP43KaqWl9V61etWjVAZ5K0dE07FO6c3y3U3ne1+k7gxLFxa1pNkjRF0w6FrcCGNr0BuHysfk47C+k04L6x3UySpClZPtSKk3wQeA6wMskO4I3AHwJbkpwL3Aac1YZfAbwQ2A58A3jlUH1JkvZssFCoqpfuYdHpC4wt4FVD9SJJmoxXNEuSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJI6Q0GS1BkKkqTOUJAkdYaCJKkzFCRJnaEgSeoMBUlSZyhIkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJHWGgiSpMxQkSZ2hIEnqDAVJUmcoSJK6RRUKSZ6f5ItJtic5b9b9SNJSs2hCIcky4N3AC4CTgJcmOWm2XUnS0rJoQgE4FdheVbdU1T8BHwLOmHFPkrSkLJ91A2NWA7ePze8Afnj3QUk2Ahvb7ANJvjiF3paKlcDXZt3EYrDlD2bdgXbj7+a8t730YKzlaXtasJhCYSJVtQnYNOs+DkdJtlXV+ln3Ie3O383pWUy7j3YCJ47Nr2k1SdKULKZQ+BywLsnTkxwJnA1snXFPkrSkLJrdR1X1cJL/BPwtsAy4sKpunHFbS4275bRY+bs5JamqWfcgSVokFtPuI0nSjBkKkqTOUJC3F9GileTCJLuS3DDrXpYKQ2GJ8/YiWuQuAp4/6yaWEkNB3l5Ei1ZVfQK4e9Z9LCWGgha6vcjqGfUiacYMBUlSZyjI24tI6gwFeXsRSZ2hsMRV1cPA/O1Fbga2eHsRLRZJPgh8Cvi+JDuSnDvrng533uZCktS5pSBJ6gwFSVJnKEiSOkNBktQZCpKkzlCQJpTku5J8KMk/JrkmyRVJnuEdPHU4WTSP45QWsyQBLgM2V9XZrfZM4PiZNiYdZG4pSJN5LvDNqvrz+UJVXcfYzQSTrE3yv5P8fXs9q9VPSPKJJNcmuSHJjyVZluSiNv+FJL85/a8kPZpbCtJkvh+4Zh9jdgE/VVUPJVkHfBBYD7wM+NuqOr89v+KJwMnA6qr6foAkK4ZqXNofhoJ08BwB/GmSk4FvAc9o9c8BFyY5AvhwVV2b5Bbgu5O8C/gb4KOzaFjanbuPpMncCPzbfYz5TeBO4JmMthCOhP6gmB9ndPfZi5KcU1X3tHFXA78KvHeYtqX9YyhIk/kY8PgkG+cLSX6AR952/Bjgjqr6NvAKYFkb9zTgzqr6C0Z//E9JshJ4XFVdCrwBOGU6X0PaO3cfSROoqkry74A/SfLbwEPArcBrxob9GXBpknOAjwAPtvpzgP+a5JvAA8A5jJ5u974k8/+YvW7o7yBNwrukSpI6dx9JkjpDQZLUGQqSpM5QkCR1hoIkqTMUJEmdoSBJ6v4ZFiEREXtnaYAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot('Class', data=df, palette=colors)\n",
    "plt.title(\"0: No Fraud || 1:Fraud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab843362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    492\n",
       "1    492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df[df.columns[:-2]]\n",
    "Y=df['Class']\n",
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48816552",
   "metadata": {
    "id": "48816552"
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6ead22",
   "metadata": {
    "id": "2e6ead22"
   },
   "source": [
    "## 3. Class를 종속변수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb699ae0",
   "metadata": {
    "executionInfo": {
     "elapsed": 798,
     "status": "ok",
     "timestamp": 1620908261886,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "cb699ae0"
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "X = df.values[:,0:30]\n",
    "Y = df.values[:,30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ed960",
   "metadata": {},
   "source": [
    "## 4. Train Set : Test set = 85:15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70420ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6792f",
   "metadata": {},
   "source": [
    "## 5. ANN/DNN활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b005696",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 258196,
     "status": "error",
     "timestamp": 1620908671189,
     "user": {
      "displayName": "강소현",
      "photoUrl": "",
      "userId": "12526461006319002709"
     },
     "user_tz": -540
    },
    "id": "3b005696",
    "outputId": "50445b54-393e-45c0-b516-26a04c5d3fe5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 1.2759 - accuracy: 0.5115 - val_loss: 0.9241 - val_accuracy: 0.4841\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.92415, saving model to ./model\\01-0.9241,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\01-0.9241,hdf5\\assets\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.8384 - accuracy: 0.5548 - val_loss: 0.6775 - val_accuracy: 0.6905\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.92415 to 0.67755, saving model to ./model\\02-0.6775,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\02-0.6775,hdf5\\assets\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6545 - accuracy: 0.7307 - val_loss: 0.5560 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.67755 to 0.55602, saving model to ./model\\03-0.5560,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\03-0.5560,hdf5\\assets\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5309 - accuracy: 0.8552 - val_loss: 0.4937 - val_accuracy: 0.8730\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.55602 to 0.49369, saving model to ./model\\04-0.4937,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\04-0.4937,hdf5\\assets\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4736 - accuracy: 0.8800 - val_loss: 0.4489 - val_accuracy: 0.8730\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.49369 to 0.44889, saving model to ./model\\05-0.4489,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\05-0.4489,hdf5\\assets\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4160 - accuracy: 0.9127 - val_loss: 0.4097 - val_accuracy: 0.8889\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.44889 to 0.40966, saving model to ./model\\06-0.4097,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\06-0.4097,hdf5\\assets\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3840 - accuracy: 0.9186 - val_loss: 0.3767 - val_accuracy: 0.8968\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.40966 to 0.37668, saving model to ./model\\07-0.3767,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\07-0.3767,hdf5\\assets\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3374 - accuracy: 0.9276 - val_loss: 0.3474 - val_accuracy: 0.8968\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.37668 to 0.34736, saving model to ./model\\08-0.3474,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\08-0.3474,hdf5\\assets\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3135 - accuracy: 0.9098 - val_loss: 0.3214 - val_accuracy: 0.8968\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.34736 to 0.32138, saving model to ./model\\09-0.3214,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\09-0.3214,hdf5\\assets\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2829 - accuracy: 0.9236 - val_loss: 0.2982 - val_accuracy: 0.8968\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.32138 to 0.29821, saving model to ./model\\10-0.2982,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\10-0.2982,hdf5\\assets\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2575 - accuracy: 0.9306 - val_loss: 0.2797 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.29821 to 0.27975, saving model to ./model\\11-0.2797,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\11-0.2797,hdf5\\assets\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2338 - accuracy: 0.9373 - val_loss: 0.2642 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.27975 to 0.26415, saving model to ./model\\12-0.2642,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\12-0.2642,hdf5\\assets\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2002 - accuracy: 0.9470 - val_loss: 0.2517 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.26415 to 0.25170, saving model to ./model\\13-0.2517,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\13-0.2517,hdf5\\assets\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2180 - accuracy: 0.9289 - val_loss: 0.2407 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.25170 to 0.24069, saving model to ./model\\14-0.2407,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\14-0.2407,hdf5\\assets\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2067 - accuracy: 0.9234 - val_loss: 0.2313 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.24069 to 0.23133, saving model to ./model\\15-0.2313,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\15-0.2313,hdf5\\assets\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1958 - accuracy: 0.9300 - val_loss: 0.2235 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.23133 to 0.22349, saving model to ./model\\16-0.2235,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\16-0.2235,hdf5\\assets\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1632 - accuracy: 0.9486 - val_loss: 0.2181 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.22349 to 0.21806, saving model to ./model\\17-0.2181,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\17-0.2181,hdf5\\assets\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1594 - accuracy: 0.9456 - val_loss: 0.2119 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.21806 to 0.21189, saving model to ./model\\18-0.2119,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\18-0.2119,hdf5\\assets\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1604 - accuracy: 0.9449 - val_loss: 0.2056 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.21189 to 0.20562, saving model to ./model\\19-0.2056,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\19-0.2056,hdf5\\assets\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1653 - accuracy: 0.9440 - val_loss: 0.2018 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.20562 to 0.20176, saving model to ./model\\20-0.2018,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\20-0.2018,hdf5\\assets\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1405 - accuracy: 0.9542 - val_loss: 0.2008 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.20176 to 0.20079, saving model to ./model\\21-0.2008,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\21-0.2008,hdf5\\assets\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1384 - accuracy: 0.9583 - val_loss: 0.1986 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.20079 to 0.19861, saving model to ./model\\22-0.1986,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\22-0.1986,hdf5\\assets\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1452 - accuracy: 0.9552 - val_loss: 0.1970 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.19861 to 0.19698, saving model to ./model\\23-0.1970,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\23-0.1970,hdf5\\assets\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1153 - accuracy: 0.9643 - val_loss: 0.1949 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.19698 to 0.19489, saving model to ./model\\24-0.1949,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\24-0.1949,hdf5\\assets\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1318 - accuracy: 0.9515 - val_loss: 0.1904 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.19489 to 0.19040, saving model to ./model\\25-0.1904,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\25-0.1904,hdf5\\assets\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1202 - accuracy: 0.9598 - val_loss: 0.1880 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.19040 to 0.18796, saving model to ./model\\26-0.1880,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\26-0.1880,hdf5\\assets\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1211 - accuracy: 0.9590 - val_loss: 0.1860 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.18796 to 0.18597, saving model to ./model\\27-0.1860,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\27-0.1860,hdf5\\assets\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1175 - accuracy: 0.9544 - val_loss: 0.1843 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.18597 to 0.18434, saving model to ./model\\28-0.1843,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\28-0.1843,hdf5\\assets\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1291 - accuracy: 0.9539 - val_loss: 0.1794 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.18434 to 0.17944, saving model to ./model\\29-0.1794,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\29-0.1794,hdf5\\assets\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1295 - accuracy: 0.9500 - val_loss: 0.1782 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.17944 to 0.17821, saving model to ./model\\30-0.1782,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\30-0.1782,hdf5\\assets\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1130 - accuracy: 0.9596 - val_loss: 0.1797 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.17821\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1151 - accuracy: 0.9590 - val_loss: 0.1799 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.17821\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0996 - accuracy: 0.9607 - val_loss: 0.1786 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.17821\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1075 - accuracy: 0.9581 - val_loss: 0.1759 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.17821 to 0.17585, saving model to ./model\\34-0.1759,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\34-0.1759,hdf5\\assets\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1085 - accuracy: 0.9655 - val_loss: 0.1754 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.17585 to 0.17536, saving model to ./model\\35-0.1754,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\35-0.1754,hdf5\\assets\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1076 - accuracy: 0.9616 - val_loss: 0.1770 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.17536\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1008 - accuracy: 0.9655 - val_loss: 0.1754 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.17536\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0923 - accuracy: 0.9689 - val_loss: 0.1715 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.17536 to 0.17147, saving model to ./model\\38-0.1715,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\38-0.1715,hdf5\\assets\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1086 - accuracy: 0.9613 - val_loss: 0.1704 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.17147 to 0.17039, saving model to ./model\\39-0.1704,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\39-0.1704,hdf5\\assets\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1030 - accuracy: 0.9658 - val_loss: 0.1728 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.17039\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0927 - accuracy: 0.9648 - val_loss: 0.1746 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.17039\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0986 - accuracy: 0.9656 - val_loss: 0.1759 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.17039\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0884 - accuracy: 0.9685 - val_loss: 0.1750 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.17039\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0940 - accuracy: 0.9625 - val_loss: 0.1719 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.17039\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0953 - accuracy: 0.9708 - val_loss: 0.1710 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.17039\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0883 - accuracy: 0.9706 - val_loss: 0.1698 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.17039 to 0.16981, saving model to ./model\\46-0.1698,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\46-0.1698,hdf5\\assets\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0820 - accuracy: 0.9736 - val_loss: 0.1697 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.16981 to 0.16970, saving model to ./model\\47-0.1697,hdf5\n",
      "INFO:tensorflow:Assets written to: ./model\\47-0.1697,hdf5\\assets\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0753 - accuracy: 0.9753 - val_loss: 0.1721 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.16970\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0778 - accuracy: 0.9689 - val_loss: 0.1750 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.16970\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0902 - accuracy: 0.9701 - val_loss: 0.1779 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.16970\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0731 - accuracy: 0.9751 - val_loss: 0.1814 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.16970\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0672 - accuracy: 0.9735 - val_loss: 0.1841 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.16970\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0904 - accuracy: 0.9677 - val_loss: 0.1818 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.16970\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0858 - accuracy: 0.9738 - val_loss: 0.1830 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.16970\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0885 - accuracy: 0.9646 - val_loss: 0.1846 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.16970\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.9734 - val_loss: 0.1871 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.16970\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0697 - accuracy: 0.9774 - val_loss: 0.1894 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.16970\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0756 - accuracy: 0.9756 - val_loss: 0.1906 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.16970\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0693 - accuracy: 0.9796 - val_loss: 0.1916 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.16970\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0683 - accuracy: 0.9802 - val_loss: 0.1927 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.16970\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0666 - accuracy: 0.9776 - val_loss: 0.1949 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.16970\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0798 - accuracy: 0.9730 - val_loss: 0.1951 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.16970\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0636 - accuracy: 0.9795 - val_loss: 0.1968 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.16970\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0672 - accuracy: 0.9801 - val_loss: 0.1975 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.16970\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0636 - accuracy: 0.9781 - val_loss: 0.1947 - val_accuracy: 0.9206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00065: val_loss did not improve from 0.16970\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0821 - accuracy: 0.9759 - val_loss: 0.1990 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.16970\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0605 - accuracy: 0.9876 - val_loss: 0.2008 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.16970\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.9756 - val_loss: 0.2017 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.16970\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0582 - accuracy: 0.9850 - val_loss: 0.2028 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.16970\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.9852 - val_loss: 0.2008 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.16970\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.9810 - val_loss: 0.2020 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.16970\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.9817 - val_loss: 0.2057 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.16970\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0541 - accuracy: 0.9855 - val_loss: 0.2118 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.16970\n",
      "Epoch 74/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0641 - accuracy: 0.9868 - val_loss: 0.2140 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.16970\n",
      "Epoch 75/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0628 - accuracy: 0.9848 - val_loss: 0.2165 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.16970\n",
      "Epoch 76/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0594 - accuracy: 0.9853 - val_loss: 0.2166 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.16970\n",
      "Epoch 77/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0583 - accuracy: 0.9853 - val_loss: 0.2190 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.16970\n",
      "Epoch 78/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0492 - accuracy: 0.9881 - val_loss: 0.2203 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.16970\n",
      "Epoch 79/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.9829 - val_loss: 0.2219 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.16970\n",
      "Epoch 80/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0559 - accuracy: 0.9863 - val_loss: 0.2228 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.16970\n",
      "Epoch 81/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.9850 - val_loss: 0.2236 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.16970\n",
      "Epoch 82/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0542 - accuracy: 0.9839 - val_loss: 0.2251 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.16970\n",
      "Epoch 83/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0650 - accuracy: 0.9829 - val_loss: 0.2276 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.16970\n",
      "Epoch 84/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0402 - accuracy: 0.9915 - val_loss: 0.2271 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.16970\n",
      "Epoch 85/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0513 - accuracy: 0.9882 - val_loss: 0.2280 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.16970\n",
      "Epoch 86/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0445 - accuracy: 0.9921 - val_loss: 0.2339 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.16970\n",
      "Epoch 87/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0467 - accuracy: 0.9917 - val_loss: 0.2328 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.16970\n",
      "Epoch 88/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0526 - accuracy: 0.9863 - val_loss: 0.2328 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.16970\n",
      "Epoch 89/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0425 - accuracy: 0.9885 - val_loss: 0.2333 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.16970\n",
      "Epoch 90/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0478 - accuracy: 0.9856 - val_loss: 0.2351 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.16970\n",
      "Epoch 91/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0358 - accuracy: 0.9918 - val_loss: 0.2352 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.16970\n",
      "Epoch 92/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0458 - accuracy: 0.9894 - val_loss: 0.2382 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.16970\n",
      "Epoch 93/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0388 - accuracy: 0.9913 - val_loss: 0.2459 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.16970\n",
      "Epoch 94/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0384 - accuracy: 0.9865 - val_loss: 0.2483 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.16970\n",
      "Epoch 95/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0392 - accuracy: 0.9899 - val_loss: 0.2507 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.16970\n",
      "Epoch 96/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0318 - accuracy: 0.9928 - val_loss: 0.2510 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.16970\n",
      "Epoch 97/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0469 - accuracy: 0.9906 - val_loss: 0.2508 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.16970\n",
      "Epoch 98/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0466 - accuracy: 0.9859 - val_loss: 0.2509 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.16970\n",
      "Epoch 99/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0375 - accuracy: 0.9927 - val_loss: 0.2541 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.16970\n",
      "Epoch 100/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0392 - accuracy: 0.9925 - val_loss: 0.2500 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.16970\n",
      "Epoch 101/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0369 - accuracy: 0.9916 - val_loss: 0.2516 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.16970\n",
      "Epoch 102/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0362 - accuracy: 0.9905 - val_loss: 0.2570 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.16970\n",
      "Epoch 103/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0326 - accuracy: 0.9948 - val_loss: 0.2612 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.16970\n",
      "Epoch 104/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0435 - accuracy: 0.9904 - val_loss: 0.2620 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.16970\n",
      "Epoch 105/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0313 - accuracy: 0.9929 - val_loss: 0.2636 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.16970\n",
      "Epoch 106/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0354 - accuracy: 0.9905 - val_loss: 0.2615 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.16970\n",
      "Epoch 107/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0386 - accuracy: 0.9910 - val_loss: 0.2614 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.16970\n",
      "Epoch 108/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0388 - accuracy: 0.9911 - val_loss: 0.2679 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.16970\n",
      "Epoch 109/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0375 - accuracy: 0.9912 - val_loss: 0.2725 - val_accuracy: 0.9206\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.16970\n",
      "Epoch 110/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0398 - accuracy: 0.9916 - val_loss: 0.2733 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.16970\n",
      "Epoch 111/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0373 - accuracy: 0.9911 - val_loss: 0.2748 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.16970\n",
      "Epoch 112/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0323 - accuracy: 0.9933 - val_loss: 0.2690 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.16970\n",
      "Epoch 113/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0398 - accuracy: 0.9901 - val_loss: 0.2713 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.16970\n",
      "Epoch 114/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0273 - accuracy: 0.9933 - val_loss: 0.2763 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.16970\n",
      "Epoch 115/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0324 - accuracy: 0.9916 - val_loss: 0.2792 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.16970\n",
      "Epoch 116/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0247 - accuracy: 0.9931 - val_loss: 0.2891 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.16970\n",
      "Epoch 117/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0355 - accuracy: 0.9914 - val_loss: 0.2919 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.16970\n",
      "Epoch 118/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0277 - accuracy: 0.9923 - val_loss: 0.2920 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.16970\n",
      "Epoch 119/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.9926 - val_loss: 0.2920 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.16970\n",
      "Epoch 120/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0264 - accuracy: 0.9953 - val_loss: 0.2938 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.16970\n",
      "Epoch 121/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0217 - accuracy: 0.9969 - val_loss: 0.3019 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.16970\n",
      "Epoch 122/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0250 - accuracy: 0.9939 - val_loss: 0.3038 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.16970\n",
      "Epoch 123/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0300 - accuracy: 0.9919 - val_loss: 0.3039 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.16970\n",
      "Epoch 124/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0305 - accuracy: 0.9934 - val_loss: 0.3056 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.16970\n",
      "Epoch 125/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0243 - accuracy: 0.9943 - val_loss: 0.3077 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.16970\n",
      "Epoch 126/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0256 - accuracy: 0.9951 - val_loss: 0.3096 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.16970\n",
      "Epoch 127/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0299 - accuracy: 0.9939 - val_loss: 0.3123 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.16970\n",
      "Epoch 128/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.9961 - val_loss: 0.3140 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.16970\n",
      "Epoch 129/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.9940 - val_loss: 0.3159 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.16970\n",
      "Epoch 130/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.3196 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.16970\n",
      "Epoch 131/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0186 - accuracy: 0.9955 - val_loss: 0.3175 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.16970\n",
      "Epoch 132/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0223 - accuracy: 0.9921 - val_loss: 0.3209 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.16970\n",
      "Epoch 133/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0184 - accuracy: 0.9964 - val_loss: 0.3277 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.16970\n",
      "Epoch 134/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.9939 - val_loss: 0.3357 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.16970\n",
      "Epoch 135/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.9934 - val_loss: 0.3382 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.16970\n",
      "Epoch 136/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.9936 - val_loss: 0.3357 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.16970\n",
      "Epoch 137/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0286 - accuracy: 0.9927 - val_loss: 0.3356 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.16970\n",
      "Epoch 138/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.9966 - val_loss: 0.3371 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.16970\n",
      "Epoch 139/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0178 - accuracy: 0.9964 - val_loss: 0.3403 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.16970\n",
      "Epoch 140/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0166 - accuracy: 0.9971 - val_loss: 0.3415 - val_accuracy: 0.9127\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.16970\n",
      "Epoch 141/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0152 - accuracy: 0.9970 - val_loss: 0.3432 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.16970\n",
      "Epoch 142/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0139 - accuracy: 0.9980 - val_loss: 0.3470 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.16970\n",
      "Epoch 143/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0196 - accuracy: 0.9961 - val_loss: 0.3464 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.16970\n",
      "Epoch 144/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0157 - accuracy: 0.9975 - val_loss: 0.3479 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.16970\n",
      "Epoch 145/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.3509 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.16970\n",
      "Epoch 146/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 0.9941 - val_loss: 0.3526 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.16970\n",
      "Epoch 147/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.9926 - val_loss: 0.3560 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.16970\n",
      "테스트 손실값 : 0.14352816343307495, 테스트 정확도 : 0.9662162065505981\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30, input_dim=30, activation='relu'))\n",
    "model.add(Dense(18, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 저장 폴더 설정\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "# 모델 저장 방법\n",
    "model_path='./model/{epoch:02d}-{val_loss:.4f},hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "# 학습 조기 종료\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
    "history = model.fit(X_train, Y_train, validation_split=0.15,epochs = 200, batch_size = 100, callbacks=[early_stopping_callback,checkpointer])\n",
    "score = model.evaluate(X_test, Y_test, verbose = 0)\n",
    "\n",
    "print(f'테스트 손실값 : {score[0]}, 테스트 정확도 : {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eda27de9",
   "metadata": {
    "id": "eda27de9",
    "outputId": "ddc1df91-57b2-410d-bb07-2719282bc855",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 18)                558       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 12)                228       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,729\n",
      "Trainable params: 1,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "951761ad",
   "metadata": {
    "id": "951761ad",
    "outputId": "fda90a61-9164-4819-c6ff-99e812b3704f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history\n",
    "# 딕셔너리 형태\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5778e887",
   "metadata": {
    "id": "5778e887",
    "outputId": "bc990525-34af-48c5-ed3f-fbfa43e57480",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "y_value = history.history['val_loss']  # 오차값을 저장\n",
    "y_acc = history.history['accuracy']    # 정밀도를 저장\n",
    "\n",
    "print(len(y_value))\n",
    "print(len(y_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "95774024",
   "metadata": {
    "id": "95774024",
    "outputId": "143b0849-9f58-49ec-9997-e99a7621da49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16f210bd070>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_len = np.arange(len(y_acc))\n",
    "\n",
    "plt.plot(x_len, y_value, c='red', markersize=3)\n",
    "plt.plot(x_len, y_acc, c='blue', markersize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "900f504e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_one = df[\"Class\"] == 1\n",
    "all_class_1 = df[class_one]\n",
    "\n",
    "all_class_1.drop('Class', axis=1, inplace = True)\n",
    "all_class_1.drop('scaled_time', axis=1, inplace = True)\n",
    "all_class_1.drop('loged_time', axis=1, inplace = True)\n",
    "all_class_1.drop('std_time', axis=1, inplace = True)\n",
    "all_class_1\n",
    "\n",
    "dt_one = pd.DataFrame(model.predict(all_class_1))\n",
    "plt.hist(dt_one, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "98d77c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.000000'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(X_test)\n",
    "format(prediction[0][0],'f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f88cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "module5_time_정규화_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
